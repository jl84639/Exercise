---
title: "HW3_no.1,4"
author: " Fan Ye,Xiangmeng Qin, Jinming Li"
date: "2024-04-01"
output: html_document
---
## PROBLEM1 WHAT CAUSES WHAT
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
1.
You can't because of several reasons: 
There might be reverse causality, which means high crime rates may lead to increased police presence, not necessarily the other way around. Also, many factors, like economic conditions and social environment, also influence crime rates. Not accounting for these can blur the true relationship between police numbers and crime.

2.
The study introduced an instrumental variable (IV) through adding the terror alert system. Higher terror alerts result in a higher police presence, which is independent of the actual crime levels in the area. The regression analysis results suggested that a high terror alert correlates with a reduction of approximately 7 crimes. When adjusting for ridership, the high terror alert is associated with a decrease in crime by roughly 6 crimes.

3.
They control for metro ridership because they want to make sure the observed decline in crime rates during high terror alerts was not merely due to reduced street activity. They were trying to capture the impact of increased terror alerts on the amount of people in the city.

4.
The first column of the table shows a robust linear regression model with three coefficients. One coefficient measures how much high terror rates affect just the first police district, where the National Mall is. Since this area is a likely target for terrorist attacks in Washington, D.C., it's given special attention. Another coefficient looks at how these high alerts affect other police districts in D.C. There's also a coefficient that considers how many people ride the metro at noon.
Conclusion:
What the data shows is that when there's a high terror alert, and more police are around, there's a big drop in crime at the National Mall. In other parts of the city, crime also goes down, but not by as much. Overall, though, the numbers show us that having more police around, especially where they expect the most trouble, does a good job of keeping crime down.

```{r setup start, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(lubridate)
library(modelr)
library(gbm)
library(caret)
library(ggmap)
library(maps)
library(mapdata)
```

## Problem 2 Tree Modeling: Dengue Cases
### Part 1: CART

```{r 1.1, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Load the dataset
dengue <- read.csv('dengue.csv')

# Calculate the number of missing values before and after removal
original_na_count <- sum(is.na(dengue))
dengue <- na.omit(dengue)
cleaned_na_count <- sum(is.na(dengue))

# Convert character variables to factors
dengue$city <- as.factor(dengue$city)
dengue$season <- as.factor(dengue$season)

# Split the dataset into training and testing sets using the tidymodels package
set.seed(123) # Ensure reproducibility of the split
dengue_split <- initial_split(dengue, prop = 0.8)
dengue_train <- training(dengue_split)
dengue_test <- testing(dengue_split)

# Build the base model (CART decision tree)
dengue_tree_CART <- rpart(total_cases ~ ., data = dengue_train, 
                          control = rpart.control(cp = 0.002, minsplit = 30))

# Output the structure and detailed splits of the tree
print(dengue_tree_CART)  # Structure
summary(dengue_tree_CART)  # More details

# Plot the decision tree
rpart.plot(dengue_tree_CART, digits = -5, type = 4, extra = 1)



```

To perform cross-validation, I used prune function given during the class and adopted in my base model to evaluate performance.

```{r 1.2.CART.2, message=FALSE, echo=FALSE, warning=FALSE}

# Define a function to prune the tree using the 1-SE rule
prune_1se <- function(my_tree) {
  # Convert the cptable in the model to a data frame for easier handling
  cptable_df <- as.data.frame(my_tree$cptable)
  # Calculate the threshold as the minimum of xerror + xstd
  thresh <- min(cptable_df$xerror + cptable_df$xstd)
  # Determine the optimal cp value that meets the threshold criteria
  cp_opt <- max(cptable_df$CP[cptable_df$xerror <= thresh])
  # Prune the tree using the determined optimal cp value
  pruned_tree <- prune(my_tree, cp = cp_opt)
  return(pruned_tree)
}

# Apply the pruning function to the CART model we created
dengue_tree_pruned <- prune_1se(dengue_tree_CART)

# Plot the pruned decision tree

rpart.plot(dengue_tree_pruned, digits = -5, type = 4, extra = 1)
```

We will proceed to prune and un-pruned  and then calculate RMSE.

```{r 1.3, message=FALSE, echo=FALSE, warning=FALSE}
# check the rmse
rmse_CART_nonprune=rmse(dengue_tree_CART,dengue_test)
rmse_CART_prune=rmse(dengue_tree_pruned,dengue_test)

# print the rmse
rmse_CART_nonprune
rmse_CART_prune
```

The result indicates that the pruned CART model exhibits a slightly higher RMSE (Root Mean Square Error) than the unpruned CART model. This outcome stems from the pruned CART's increased bias and decreased variance, attributed to its reduced flexibility compared to the unpruned CART.

### Part 2: Random Forest

```{r 2.1, message=FALSE, echo=FALSE, warning=FALSE}

#random forest
DengueRandom = randomForest(total_cases ~ ., data= dengue_train, importance = TRUE)
plot(DengueRandom)
```

This graph displays the relationship between the number of trees utilized and the out-of-bag mean squared error (MSE). Next, we will examine how the root mean square error (RMSE) performs against the test set.

```{r 2.2, message=FALSE, echo=FALSE, warning=FALSE}

rmse_random = rmse(DengueRandom, dengue_test)
cat(rmse_random,' RMSE for Random Forest')
```
### Part 3: Gradient Boosted Trees

After fitting the gradient boosted trees model, I implemented cross-validation using the `cv.folds()` function within the `gbm` package. Based on my research, the common range for the number of folds in cross-validation is between 5 and 10, so I opted for 7 folds.

Following this, I generated an error curve, specifically a deviance plot, to visualize the model's performance across different data subsets during the cross-validation process, thereby offering insights into the error characteristics of the model.

```{r 3.1, message=FALSE, echo=FALSE, warning=FALSE}

# fit a tree for gradient boosted trees
#boosted trees
dengue_tree_Boost = gbm(total_cases ~ ., data= dengue_train,
             interaction.depth=4, n.trees=350, shrinkage=.05, cv.folds = 7, 
             distribution='gaussian')

# Look at error curve -- stops decreasing much after ~300
gbm.perf(dengue_tree_Boost)
```

The green line represents the error obtained through cross-validation. On the error curve, the x-axis denotes the number of iterations, while the y-axis indicates the model's deviance, serving as a metric for the fit's quality. The blue dashed line marks the optimal iteration count that minimizes error.

This plot illustrates the error trajectory for the Gradient Boosted Model, indicating the ideal tree count as part of the results.

Next, we will evaluate the RMSE for the Gradient Boosted Trees Model.

```{r 3.2, message=FALSE, echo=FALSE, warning=FALSE}

# check the rmse
rmse_Boost=rmse(dengue_tree_Boost,dengue_test)
cat(rmse_Boost,' RMSE for Gradient Boosted Trees') 
```

### Part 4:  Checking model performance 

```{r 4.1, message=FALSE, echo=FALSE, warning=FALSE}

# Define model names and corresponding RMSE values
models <- c("Un-pruned Tree", "Pruned Tree", "Random Forest", "Gradient Boosted")
rmse_values <- c(rmse_CART_nonprune, rmse_CART_prune, rmse_random, rmse_Boost)

# Create a dataframe with named columns
model_performance <- data.frame(Model = models, RMSE = rmse_values)

# Format the table output using the knitr package
knitr::kable(model_performance, col.names = c("Model", "RMSE"))



```

Examining the RMSE outcomes for the three models, the random forest emerges as the most suitable option for this specific dataset. 

Subsequently, we will present the partial dependency plots for the Random Forest Model.


### Part 5: Partial Dependency Plots

With the chosen top-performing model, Random Forest, we aim to visualize partial dependence plots to assess the isolated impact of certain features on the model outcome.

We will generate three partial dependence plots focusing on `specific_humidity`, `precipitation_amt`, and our team's selected variable `tdtr_k`. This particular variable was identified through a variable importance plot derived from the Random Forest analysis. 

These plots will provide insights into the individual effects of these features on the model's predictions, allowing for a clearer understanding of their influence on the outcome.

```{r 5.1, message=FALSE, echo=FALSE, warning=FALSE}

partialPlot(DengueRandom, dengue_test, 'specific_humidity', las=1)
partialPlot(DengueRandom, dengue_test, 'precipitation_amt', las=1)
partialPlot(DengueRandom, dengue_test, 'tdtr_k', las=1)

```

Observing the partial dependence plots, we can conclude that the more standing water there is, the greater the number of mosquitoes and consequently, the more cases of dengue fever. Increased precipitation leads to more standing water, which in turn increases the mosquito population. The resulting increase in mosquitoes leads to more cases of dengue fever, which is reasonable. Humidity is a measure of the amount of evaporated moisture in the air. Higher humidity levels indicate more moisture on the ground, providing more breeding grounds for mosquitoes. As a result, an increase in humidity leads to an increase in the mosquito population and subsequently, an increase in dengue fever cases. However, a larger diurnal temperature range may affect mosquito survival and reproduction due to temperature fluctuations, resulting in a decrease in the mosquito population and consequently, a decrease in dengue fever cases.

## PROBLEM3 Predictive model building: green certification
Data Preparation
The dataset comprised 7,894 commercial rental properties across the United States, with variables including building size, employment growth rate, rent, leasing rate, and green certification status (LEED, EnergyStar). The target variable for prediction was the revenue per square foot per year, calculated as the product of rent and leasing rate.
We conducted the following steps:Split the data into training and testing sets.
Fitted the model on the training set.Predicted revenue on the test set.
Calculated the Root Mean Squared Error (RMSE) to evaluate model performance.
To build the best predictive model possible for revenue per square foot per calendar year, and to use this model to quantify the average change in rental income per square foot (whether in absolute or percentage terms) associated with green certification, holding other features of the building constant, we will test stepwise selction model,Classification and Regression Trees model, bagging method model, random forest model, and ridge regression method model, then compare their RMSE.
We use Rent - leasing_rate - CS_PropertyID - LEED - Energystar as the variables to train the models.
```{r Q3.1, message=FALSE, echo=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(tidyverse)
library(mosaic)
library(dplyr)
library(data.table)
library(rsample)
library(modelr)
library(ggplot2)
library(rpart)
library(ipred)
library(caret)
library(randomForest)
library(gbm)
library(pdp)
library(ggplot2)
library(xgboost)
library(Metrics)
library(purrr)
library(glmnet)
library(caret)
library(rpart.plot)
# Data preparation
greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")
greenbuildings$green_certified <- with(greenbuildings, LEED | Energystar)
greenbuildings$revenue_per_sqft = greenbuildings$Rent * greenbuildings$leasing_rate
# Splitting data
greenbuildings_split = initial_split(greenbuildings, prop = 0.8)
greenbuildings_train = training(greenbuildings_split)
greenbuildings_test = testing(greenbuildings_split)
```
Firstly, we try the linear regression model, we use the Rent - leasing_rate - CS_PropertyID - LEED - Energystar to run the regression:
```{r Q3.2, message=FALSE, echo=FALSE, warning=FALSE}
# Linear Regression
model_lr <- lm(revenue_per_sqft ~ . - Rent - leasing_rate - CS_PropertyID - LEED - Energystar, data=greenbuildings_train)
# Predicting revenues using the linear regression model
predicted_revenues_lr <- predict(model_lr, newdata = greenbuildings_test)
# Create a data frame with actual and predicted values
results_df <- data.frame(Actual = greenbuildings_test$revenue_per_sqft, Predicted = predicted_revenues_lr)
# Plot
ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +  # Plot actual vs predicted as points
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = 'red') +  # Perfect predictions line
  labs(title = "Actual vs. Predicted Revenue per SqFt (Linear Regression)", x = "Actual Revenue per SqFt", y = "Predicted Revenue per SqFt") +
  theme_minimal()
plot(model_lr)
# Calculate RMSE
non_na_indices <- !is.na(greenbuildings_test$revenue_per_sqft) & !is.na(predicted_revenues_lr)
rmse_lr <- rmse(greenbuildings_test$revenue_per_sqft[non_na_indices], predicted_revenues_lr[non_na_indices])
# Print RMSE
cat("RMSE:", rmse_lr, "\n")
```
The run the Stepwise selection model:
```{r Q3.31, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
# Stepwise selection model
greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")
greenbuildings$green_certified <- with(greenbuildings, LEED | Energystar)
greenbuildings$revenue_per_sqft = greenbuildings$Rent * greenbuildings$leasing_rate
greenbuildings_split = initial_split(greenbuildings, prop = 0.8)
greenbuildings_train = training(greenbuildings_split)
greenbuildings_test = testing(greenbuildings_split)
lm_basic = lm(revenue_per_sqft ~. -CS_PropertyID -LEED -Energystar -Rent -leasing_rate, data=greenbuildings_train)
lm_step = step(lm_basic, direction ="both")
predicted_revenues_stepwise <- predict(lm_step, newdata = greenbuildings_test)
```

```{r Q3.32, message=FALSE, echo=FALSE, warning=FALSE}
# Plotting actual vs. predicted revenues
ggplot() +
  geom_point(aes(x = greenbuildings_test$revenue_per_sqft, y = predicted_revenues_stepwise), colour = "blue") +
  geom_line(aes(x = greenbuildings_test$revenue_per_sqft, y = greenbuildings_test$revenue_per_sqft), colour = "red") +
  labs(x = "Actual Revenue", y = "Predicted Revenue", title = "Stepwise Selection Model: Predicted vs Actual Revenue") +
  theme_minimal()
plot(lm_step)
# Calculate RMSE
non_na_indices <- !is.na(greenbuildings_test$revenue_per_sqft) & !is.na(predicted_revenues_stepwise)
rmse_stepwise <- rmse(greenbuildings_test$revenue_per_sqft[non_na_indices], predicted_revenues_stepwise[non_na_indices])
# Print RMSE
cat("RMSE:", rmse_stepwise, "\n")
```
Then, the  CART model:
```{r Q3.41, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
#Classification and Regression Trees (CART)
set.seed(1)
Tree1 = rpart(revenue_per_sqft ~ . -CS_PropertyID -LEED -Energystar -Rent -leasing_rate, data=greenbuildings_train, method = "anova")
predicted_revenues_cart <- predict(Tree1, newdata = greenbuildings_test)
```

```{r Q3.42, message=FALSE, echo=FALSE, warning=FALSE, }
# Plotting
rpart.plot(Tree1, type = 4, extra = 1)
ggplot() +
  geom_point(aes(x = greenbuildings_test$revenue_per_sqft, y = predicted_revenues_cart), colour = "blue") +
  geom_line(aes(x = greenbuildings_test$revenue_per_sqft, y = greenbuildings_test$revenue_per_sqft), colour = "red") +
  labs(x = "Actual Revenue", y = "Predicted Revenue", title = "CART Model: Predicted vs Actual Revenue") +
  theme_minimal()
# Calculate RMSE
non_na_indices <- !is.na(greenbuildings_test$revenue_per_sqft) & !is.na(predicted_revenues_cart)
rmse_cart <- rmse(greenbuildings_test$revenue_per_sqft[non_na_indices], predicted_revenues_cart[non_na_indices])
# Print RMSE
cat("RMSE:", rmse_cart, "\n")
```
Then try the Bagging Method:
```{r Q3.5, message=FALSE, echo=FALSE, warning=FALSE}
#Bagging Method
set.seed(1)
Tree2 = bagging(revenue_per_sqft ~ . -CS_PropertyID -LEED -Energystar -Rent -leasing_rate, data=greenbuildings, nbagg=150, coob=T)
# Predicted revenues from Bagging
predicted_revenues_bagging <- predict(Tree2, newdata = greenbuildings_test)
# Plotting
ggplot() +
  geom_point(aes(x = greenbuildings_test$revenue_per_sqft, y = predicted_revenues_bagging), colour = "blue") +
  geom_line(aes(x = greenbuildings_test$revenue_per_sqft, y = greenbuildings_test$revenue_per_sqft), colour = "red") +
  labs(x = "Actual Revenue", y = "Predicted Revenue", title = "Bagging Model: Predicted vs Actual Revenue") +
  theme_minimal()
# Calculate RMSE
non_na_indices <- !is.na(greenbuildings_test$revenue_per_sqft) & !is.na(predicted_revenues_bagging)
rmse_bagging <- rmse(greenbuildings_test$revenue_per_sqft[non_na_indices], predicted_revenues_bagging[non_na_indices])
# Print RMSE
cat("RMSE:", rmse_bagging, "\n")
```
Then the Random Forest Method:
```{r Q3.61, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
# Data preparation
greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv") %>%
  na.omit() %>%
  mutate(revenue = Rent * (leasing_rate/100))
greenbuildings$green_certified <- with(greenbuildings, LEED | Energystar)
greenbuildings$revenue_per_sqft = greenbuildings$Rent * greenbuildings$leasing_rate
# Splitting data
greenbuildings_split = initial_split(greenbuildings, prop = 0.8)
greenbuildings_train = training(greenbuildings_split)
greenbuildings_test = testing(greenbuildings_split)
#Random Forest Method
set.seed(1)
Tree3 = randomForest(revenue_per_sqft ~ . -CS_PropertyID -LEED -Energystar -Rent -leasing_rate, data=greenbuildings, importance=TRUE)
random_forest_predictions <- predict(Tree3, greenbuildings_test)
# Variable importance, focusing on green_certified
importance(Tree3, type=2)
# Variable importance
varImpPlot(Tree3)
# Calculate RMSE
rmse_randomforest <- rmse(greenbuildings_test$revenue_per_sqft, random_forest_predictions)
# Print RMSE
cat("RMSE:", rmse_randomforest, "\n")
```
The Ridge regression model:
```{r Q3.8, message=FALSE, echo=FALSE, warning=FALSE}
# Convert data frames to matrices for glmnet
x_train <- model.matrix(revenue_per_sqft ~ . - Rent - leasing_rate - CS_PropertyID - LEED - Energystar - 1, data = greenbuildings_train)
y_train <- greenbuildings_train$revenue_per_sqft
x_test <- model.matrix(revenue_per_sqft ~ . - Rent - leasing_rate - CS_PropertyID - LEED - Energystar - 1, data = greenbuildings_test)
y_test <- greenbuildings_test$revenue_per_sqft
# Load the glmnet package
# Fit the Ridge Regression model
set.seed(123) # For reproducibility
ridge_model <- glmnet(x_train, y_train, alpha = 0)
# Perform cross-validation to find the optimal lambda
set.seed(123) # Ensure reproducibility
cv_ridge_model <- cv.glmnet(x_train, y_train, alpha = 0)
# Predict using the optimal lambda value
predictions_ridge <- predict(cv_ridge_model, s = "lambda.min", newx = x_test)
# Calculate RMSE, ensuring predictions_ridge is correctly dimensioned
rmse_ridge <- sqrt(mean((predictions_ridge - matrix(y_test))^2))
# Print RMSE
cat("RMSE for Ridge Regression Model:", rmse_ridge, "\n")
# Plot the CV error as a function of lambda
plot(cv_ridge_model)
```
And then, do the RMSE comparison:
```{r Q3.9, message=FALSE, echo=FALSE, warning=FALSE}
rmse_values <- c(linearModel = rmse_lr,
                 Stepwise = rmse_stepwise,
                 CART = rmse_cart,
                 Bagging = rmse_bagging,
                 RandomForest = rmse_randomforest,
                 Ridge = rmse_ridge )

# Create a data frame for plotting
rmse_df <- data.frame(Model = names(rmse_values), RMSE = rmse_values)
# Plot RMSE comparison
ggplot(rmse_df, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  theme_minimal() +
  labs(title = "RMSE Comparison Among Models",
       x = "Model",
       y = "RMSE") +
  coord_flip() # Flip coordinates for horizontal bars
```
Conclusion: The randomforest model performed the best with the lowest RMSE value, indicating it's the most accurate in predicting the revenue for green-certified buildings among the models among tested. Higher relative influence values indicate that a feature is more important for predicting the outcome variable. See where green certification feature ranks among all the predictors above. Plots visualizes the effect of green certification on rental income per square foot. A higher curve for green-certified buildings compared to non-certified ones would indicate a positive effect of green certification on rental income, holding other factors constant.To quantify the green certification's impact, we predicted revenue for the dataset with all other variables held constant, comparing scenarios with and without green certification.

```{r Q3.101, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv") %>%
  na.omit() %>%
  mutate(revenue = Rent * (leasing_rate/100))
greenbuildings$green_certified <- with(greenbuildings, LEED | Energystar)
greenbuildings$revenue_per_sqft = greenbuildings$Rent * greenbuildings$leasing_rate
set.seed(123) # For reproducibility
rf_model <- randomForest(revenue_per_sqft ~ ., data = greenbuildings, importance = TRUE)
# Check variable importance
importance(rf_model)
# Create a new dataset from the original, flipping the green certification status
greenbuildings_with_cert <- greenbuildings
greenbuildings_without_cert <- greenbuildings
greenbuildings_with_cert$green_certified <- 1 # Assuming all are certified
greenbuildings_without_cert$green_certified <- 0 # Assuming none are certified
# Predict with and without green certification
predictions_with_cert <- predict(rf_model, newdata = greenbuildings_with_cert)
predictions_without_cert <- predict(rf_model, newdata = greenbuildings_without_cert)
# Calculate the average difference
average_change <- mean(predictions_with_cert - predictions_without_cert)
# Print the average change
```

```{r Q3.102, message=FALSE, echo=FALSE, warning=FALSE}
print(average_change)
```
The average change is 0.8041974 according to the randomforest model.This tells us that the green certification has a positive influence on the rental revenue. This suggests that the RandomForest model, with its ability to handle complex nonlinear relationships and interactions between predictors, is particularly well-suited for predicting rental income in the context of commercial real estate.

```{r load libraries, echo=FALSE,message=FALSE, warning=FALSE}
# Load required libraries
library(tidyverse)
library(mosaic)
library(dplyr)
library(data.table)
library(rsample)
library(modelr)
library(ggplot2)
library(rpart)
library(ipred)
library(caret)
library(randomForest)
library(gbm)
library(pdp)
library(ggmap)
```




## problem4 Predictive model building: California housing

### Data preparing

```{r data, echo=FALSE,message=FALSE, warning=FALSE}
CAhousing_data <- read.csv("CAhousing.csv")
CAhousing_split = initial_split(CAhousing_data, prop=0.8)
CAhousing_train = training(CAhousing_split)
CAhousing_test  = testing(CAhousing_split)
```

Four predictive models were built and compared to select the best performing model based on out-of-sample accuracy:

### Model 1: CART: classification and regression trees

```{r model1, echo=FALSE,message=FALSE, warning=FALSE}
CA_Cart = rpart(medianHouseValue ~ . , data=CAhousing_train, control = rpart.control(cp = 0.000001))
```

### Model 2: Bootstrap Aggregating

```{r model2, echo=FALSE,message=FALSE, warning=FALSE}
CA_ba = bagging(medianHouseValue ~ ., data = CAhousing_train, nbagg = 150, coob = TRUE)
```

### Model 3:GBM Model
```{r model3, echo=FALSE,message=FALSE, warning=FALSE}
CA_GBM = gbm(medianHouseValue ~., 
               data = CAhousing_train,
               distribution = "gaussian",
               interaction.depth=4, n.trees=4200, shrinkage=.042)
```

### Model 4: Random Forest model
```{r model4, echo=FALSE,message=FALSE, warning=FALSE}
CA_RFD = randomForest(medianHouseValue ~ . , data=CAhousing_train, control = rpart.control(cp = 0.00001), importance=TRUE)
```

Use Root Mean Square Error (RMSE) as the measure of model accuracy. The RMSE is calculated as the square root of the average squared differences between the predicted and actual median house values. Lower RMSE values indicate better model performance.

### Calculate RMSE
```{r Calculate RMSE, echo=FALSE,message=FALSE, warning=FALSE}
predictions_CA_Cart <- predict(CA_Cart, newdata = CAhousing_test)
predictions_CA_ba <- predict(CA_ba, newdata = CAhousing_test)
predictions_CA_GBM <- predict(CA_GBM, newdata = CAhousing_test, n.trees = 4200) 
predictions_CA_RFD <- predict(CA_RFD, newdata = CAhousing_test)

actual_values <- CAhousing_test$medianHouseValue

rmse_CA_Cart = sqrt(mean((predictions_CA_Cart - actual_values)^2))
rmse_CA_ba = sqrt(mean((predictions_CA_ba - actual_values)^2))
rmse_CA_GBM = sqrt(mean((predictions_CA_GBM - actual_values)^2))
rmse_CA_RFD = sqrt(mean((predictions_CA_RFD - actual_values)^2))

print(paste("RMSE CART:", rmse_CA_Cart))
print(paste("RMSE Bagging:", rmse_CA_ba))
print(paste("RMSE GBM:", rmse_CA_GBM))
print(paste("RMSE Random Forest:", rmse_CA_RFD))
```

Given the RMSE results for each model, we can see that the Gradient Boosting Machine (GBM) model has the lowest RMSE value at `r rmse_CA_GBM`, which shows it can best predicting the median house value on CA housing. 

### (1) Plot1: Original data
```{r Plot1: Original data, echo=FALSE,message=FALSE, warning=FALSE}
register_stadiamaps(key = '101bdf83-e895-42f3-97ee-3ffe1d376d8e')

california_map <- get_stadiamap(bbox = c(left = -124.48, bottom = 32.53, 
                                         right = -114.13, top = 42.01), 
                                 zoom = 6, source = "stamen", maptype = "stamen_toner_lite")

california_base_map <- ggmap(california_map)

california_base_map +
  geom_point(data = CAhousing_data, aes(x = longitude, y = latitude, color = medianHouseValue),
             size = 1, alpha = 0.6) +
  scale_color_gradient(low = "skyblue", high = "orange",
                       name = "Actual Median\nHouse Value ($K)",
                       labels = scales::label_comma(accuracy = 1000)) +
  labs(title = "Actual Median House Value in California",
       x = "Longitude", y = "Latitude",
       color = "Actual Value") +  
  theme_minimal() +
  theme(legend.position = "right",  
        legend.direction = "vertical",  
        plot.title = element_text(size = 16, face = "bold"),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 14),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10)) 
```

### (2) Plot2: Predicted data
Use GBM model to predict the data.

```{r Plot2: Predicted data2, echo=FALSE,message=FALSE, warning=FALSE}
register_stadiamaps(key = '101bdf83-e895-42f3-97ee-3ffe1d376d8e')
CA_Predict = predict(CA_GBM, CAhousing_data)
qmplot(longitude, latitude, data = CAhousing_data, 
       color = CA_Predict, size = I(1), darken = .1) +
  xlab("Longitude") + ylab("Latitude") +
  ggtitle("Predicted Median House Value in California") +
  scale_colour_gradient(low = "skyblue", high = "orange", 
                        guide = guide_legend(override.aes = list(size=4))) +  
  labs(color = "Predicted Value") +  
  theme_minimal() +  
  theme(legend.position = "bottom",  
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),  
        legend.title = element_text(size = 10),  
        legend.text = element_text(size = 8)) 
```

From the plot, we notice that higher values of houses are along the coast, particularly around major urban areas. This distribution aligns with the known trend of real estate prices.
The comparison between the actual median house values and the predicted values indicates that the GBM model has successfully capturing the general trend across the state. 

### (3) Plot3: Residuals

```{r Plot3: Residuals, echo=FALSE,message=FALSE, warning=FALSE}
register_stadiamaps(key = '101bdf83-e895-42f3-97ee-3ffe1d376d8e')

actual_values <- CAhousing_data$medianHouseValue
residuals <- actual_values - CA_Predict
CAhousing_data$PredictedValue <- CA_Predict
CAhousing_data$Residuals <- residuals

residuals_map <- ggmap(california_map) +
  geom_point(data = CAhousing_data, aes(x = longitude, y = latitude, color = Residuals),
             size = 1, alpha = 0.6) +
  scale_color_gradient(low = "blue", high = "red",
                        name = "Residuals ($K)", 
                       labels = scales::label_comma(accuracy = 1000)) +  
  labs(title = "Residuals of Predicted Median House Value in California",
       x = "Longitude", y = "Latitude",
       color = "Residuals") +  
  theme_minimal() +
  theme(legend.position = "right",  
        legend.direction = "vertical",  
        plot.title = element_text(size = 16, face = "bold"),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 14),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10))  
print(residuals_map)
```

We have both positive and negative residuals across the state, which suggests that the model has performed well in general, but there are specific places where the model can overestimates or underestimates the values. 
